{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Landing zone"
      ],
      "metadata": {
        "id": "cSppTwYP3lYu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llVKUxZMwnuc",
        "outputId": "72bf2d39-5495-4108-e834-2cf482705d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta Lake Configuration"
      ],
      "metadata": {
        "id": "eSlrj9Y23P4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark and Delta Lake for data processing and storage\n",
        "\n",
        "!pip install pyspark\n",
        "!pip install delta-spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wFFfag2wr6N",
        "outputId": "ecd8db0f-fe62-45f4-931c-c069b30926cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting delta-spark\n",
            "  Downloading delta_spark-3.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pyspark<3.6.0,>=3.5.3 in /usr/local/lib/python3.11/dist-packages (from delta-spark) (3.5.5)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from delta-spark) (8.6.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.21.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark<3.6.0,>=3.5.3->delta-spark) (0.10.9.7)\n",
            "Downloading delta_spark-3.3.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: delta-spark\n",
            "Successfully installed delta-spark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pyspark  # PySpark for distributed data processing\n",
        "from pyspark.sql import SparkSession  # SparkSession is the entry point for PySpark\n",
        "from delta import *  # Delta Lake integration for PySpark\n",
        "\n",
        "# Initialize SparkSession with Delta Lake configuration\n",
        "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "# Create SparkSession\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
      ],
      "metadata": {
        "id": "cWhuVcdDw5-b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temporary Landing"
      ],
      "metadata": {
        "id": "Gdl3E98P3BLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory containing CSV files\n",
        "csv_directory = \"/content/drive/MyDrive/Colab Notebooks/BDM25/csv_data\"\n",
        "\n",
        "# Define the path for the Temporal Landing Zone\n",
        "delta_directory = \"/content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data\"\n",
        "\n",
        "# Process all CSV files in the folder\n",
        "for csv_file in os.listdir(csv_directory):\n",
        "    if csv_file.endswith(\".csv\"):\n",
        "        file_path = os.path.join(csv_directory, csv_file)\n",
        "\n",
        "        # Read the CSV file as a PySpark DataFrame\n",
        "        df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(file_path)\n",
        "\n",
        "        # Define the Delta storage path for each file\n",
        "        delta_path = os.path.join(delta_directory, csv_file.replace(\".csv\", \"\"))\n",
        "\n",
        "        # Write data to Delta Lake (Temporal Landing Zone)\n",
        "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
        "\n",
        "        print(f\"✅ Successfully saved file {csv_file} to Delta Lake at: {delta_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MEaZYF0w71b",
        "outputId": "48fef5f7-f9f3-40d6-eaa1-476056e165cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully saved file appearances.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/appearances\n",
            "✅ Successfully saved file club_games.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/club_games\n",
            "✅ Successfully saved file clubs.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/clubs\n",
            "✅ Successfully saved file competitions.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/competitions\n",
            "✅ Successfully saved file game_events.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/game_events\n",
            "✅ Successfully saved file game_lineups.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/game_lineups\n",
            "✅ Successfully saved file games.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/games\n",
            "✅ Successfully saved file player_valuations.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/player_valuations\n",
            "✅ Successfully saved file players.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/players\n",
            "✅ Successfully saved file transfers.csv to Delta Lake at: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/transfers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attach metadata"
      ],
      "metadata": {
        "id": "JZjKb3CW3hg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the metadata file path\n",
        "metadata_path = os.path.join(delta_directory, \"metadata.json\")\n",
        "\n",
        "# Extract metadata\n",
        "def extract_metadata(dataframe, csv_file, file_path, delta_path):\n",
        "    metadata = {\n",
        "        \"file_name\": csv_file,\n",
        "        \"columns\": dataframe.columns,\n",
        "        \"num_rows\": dataframe.count(),\n",
        "        \"creation_time\": str(datetime.now()),\n",
        "        \"last_modified\": str(datetime.fromtimestamp(os.path.getmtime(file_path))),\n",
        "        \"delta_path\": delta_path\n",
        "    }\n",
        "    return metadata\n",
        "\n"
      ],
      "metadata": {
        "id": "9iUamNiGxDa2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list to store metadata\n",
        "all_metadata = []\n",
        "\n",
        "# Process all CSV files and extract metadata\n",
        "for csv_file in os.listdir(csv_directory):\n",
        "    if csv_file.endswith(\".csv\"):\n",
        "        file_path = os.path.join(csv_directory, csv_file)\n",
        "\n",
        "        # Read CSV file as PySpark DataFrame\n",
        "        df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(file_path)\n",
        "\n",
        "        # Define the Delta storage path\n",
        "        delta_path = os.path.join(delta_directory, csv_file.replace(\".csv\", \"\"))\n",
        "\n",
        "        # Extract metadata and add it to the list\n",
        "        metadata = extract_metadata(df, csv_file, file_path, delta_path)\n",
        "        all_metadata.append(metadata)\n",
        "\n",
        "# Save all metadata to a JSON file\n",
        "with open(metadata_path, \"w\") as f:\n",
        "    json.dump(all_metadata, f, indent=4)\n",
        "\n",
        "print(f\"✅ All metadata saved to: {metadata_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wptBAI8416Tn",
        "outputId": "e2a7d560-fe7c-4591-8259-ca8efd573065"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All metadata saved to: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data/metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Persistent Landing"
      ],
      "metadata": {
        "id": "VwvmohQA3YM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for the Persistent Landing Zone\n",
        "persistent_directory = \"/content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data\"\n",
        "\n",
        "# Move data from Temporal to Persistent Landing Zone\n",
        "for metadata in all_metadata:\n",
        "    csv_file = metadata[\"file_name\"]\n",
        "    delta_path = metadata[\"delta_path\"]\n",
        "\n",
        "    # Define the persistent storage path\n",
        "    persistent_path = os.path.join(persistent_directory, csv_file.replace(\".csv\", \"\"))\n",
        "\n",
        "    # Read data from the Temporal Landing Zone\n",
        "    df = spark.read.format(\"delta\").load(delta_path)\n",
        "\n",
        "    # Save data to the Persistent Landing Zone\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").save(persistent_path)\n",
        "\n",
        "    # Update metadata with the persistent path information\n",
        "    metadata[\"persistent_path\"] = persistent_path\n",
        "\n",
        "    print(f\"✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: {persistent_path}\")\n",
        "\n",
        "# Save the updated metadata to the JSON file\n",
        "with open(metadata_path, \"w\") as f:\n",
        "    json.dump(all_metadata, f, indent=4)\n",
        "\n",
        "print(f\"✅ All metadata has been updated and saved to: {persistent_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yPilj2pxYYh",
        "outputId": "e527a7bb-597d-4cde-e331-93dc80b11636"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/appearances\n",
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/club_games\n",
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/clubs\n",
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/competitions\n",
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/game_events\n",
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/game_lineups\n",
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/games\n",
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/player_valuations\n",
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/players\n",
            "✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/transfers\n",
            "✅ All metadata has been updated and saved to: /content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data/transfers\n"
          ]
        }
      ]
    }
  ]
}