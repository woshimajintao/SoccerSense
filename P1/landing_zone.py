# -*- coding: utf-8 -*-
"""Landing Zone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kaRms2CrHiYLSddKwViWumqkBbwd1gHn

# Landing zone

## Delta Lake Configuration
"""

# Install PySpark and Delta Lake for data processing and storage

!pip install pyspark
!pip install delta-spark

# Import necessary libraries
import pyspark  # PySpark for distributed data processing
from pyspark.sql import SparkSession  # SparkSession is the entry point for PySpark
from delta import *  # Delta Lake integration for PySpark

# Initialize SparkSession with Delta Lake configuration
builder = pyspark.sql.SparkSession.builder.appName("MyApp") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

# Create SparkSession
spark = configure_spark_with_delta_pip(builder).getOrCreate()

"""## Temporary Landing"""

import os

# Define the directory containing CSV files
csv_directory = "/content/drive/MyDrive/Colab Notebooks/BDM25/csv_data"

# Define the path for the Temporal Landing Zone
delta_directory = "/content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/delta_data"

# Process all CSV files in the folder
for csv_file in os.listdir(csv_directory):
    if csv_file.endswith(".csv"):
        file_path = os.path.join(csv_directory, csv_file)

        # Read the CSV file as a PySpark DataFrame
        df = spark.read.option("delimiter", ",").option("header", True).csv(file_path)

        # Define the Delta storage path for each file
        delta_path = os.path.join(delta_directory, csv_file.replace(".csv", ""))

        # Write data to Delta Lake (Temporal Landing Zone)
        df.write.format("delta").mode("overwrite").save(delta_path)

        print(f"✅ Successfully saved file {csv_file} to Delta Lake at: {delta_path}")

"""## Attach metadata"""

import json
from datetime import datetime

# Define the metadata file path
metadata_path = os.path.join(delta_directory, "metadata.json")

# Extract metadata
def extract_metadata(dataframe, csv_file, file_path, delta_path):
    metadata = {
        "file_name": csv_file,
        "columns": dataframe.columns,
        "num_rows": dataframe.count(),
        "creation_time": str(datetime.now()),
        "last_modified": str(datetime.fromtimestamp(os.path.getmtime(file_path))),
        "delta_path": delta_path
    }
    return metadata

# Create an empty list to store metadata
all_metadata = []

# Process all CSV files and extract metadata
for csv_file in os.listdir(csv_directory):
    if csv_file.endswith(".csv"):
        file_path = os.path.join(csv_directory, csv_file)

        # Read CSV file as PySpark DataFrame
        df = spark.read.option("delimiter", ",").option("header", True).csv(file_path)

        # Define the Delta storage path
        delta_path = os.path.join(delta_directory, csv_file.replace(".csv", ""))

        # Extract metadata and add it to the list
        metadata = extract_metadata(df, csv_file, file_path, delta_path)
        all_metadata.append(metadata)

# Save all metadata to a JSON file
with open(metadata_path, "w") as f:
    json.dump(all_metadata, f, indent=4)

print(f"✅ All metadata saved to: {metadata_path}")

"""## Persistent Landing"""

# Define the path for the Persistent Landing Zone
persistent_directory = "/content/drive/MyDrive/Colab Notebooks/BDM25/csv_data/persistent_data"

# Move data from Temporal to Persistent Landing Zone
for metadata in all_metadata:
    csv_file = metadata["file_name"]
    delta_path = metadata["delta_path"]

    # Define the persistent storage path
    persistent_path = os.path.join(persistent_directory, csv_file.replace(".csv", ""))

    # Read data from the Temporal Landing Zone
    df = spark.read.format("delta").load(delta_path)

    # Save data to the Persistent Landing Zone
    df.write.format("delta").mode("overwrite").save(persistent_path)

    # Update metadata with the persistent path information
    metadata["persistent_path"] = persistent_path

    print(f"✅ Data successfully moved from Temporal Landing Zone to Persistent Landing Zone: {persistent_path}")

# Save the updated metadata to the JSON file
with open(metadata_path, "w") as f:
    json.dump(all_metadata, f, indent=4)

print(f"✅ All metadata has been updated and saved to: {persistent_path}")